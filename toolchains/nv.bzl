# nix/build/toolchains/nv.bzl
#
# NVIDIA toolchain using hermetic Nix store paths.
#
# This toolchain provides NVIDIA target compilation via clang (NOT nvcc).
# Paths are read from .buckconfig.local [nv] section, generated by `nix develop`.
#
# We say "nv" not "cuda" because:
#   - "CUDA" is NVIDIA marketing speak
#   - "nv" is explicit about the target hardware
#   - The nvidia-sdk provides headers and runtime, NOT a compiler
#
# Compilation uses clang with:
#   -x cuda          Tell clang this is device code
#   --cuda-path=     Path to nvidia-sdk (headers, libcudart)
#   --cuda-gpu-arch= Target SM architecture (sm_90, sm_120, etc.)
#
# No nvcc. Ever.

NvToolchainInfo = provider(
    doc = "NVIDIA SDK configuration for Clang device compilation",
    fields = {
        "nvidia_sdk_path": provider_field(str),
        "nvidia_sdk_include": provider_field(str),
        "nvidia_sdk_lib": provider_field(str),
        "nv_archs": provider_field(list[str]),
    },
)

def _nv_toolchain_impl(ctx: AnalysisContext) -> list[Provider]:
    """
    NVIDIA toolchain with paths from .buckconfig.local.

    Reads [nv] section for absolute Nix store paths:
      nvidia_sdk_path    - nvidia-sdk root (for --cuda-path)
      nvidia_sdk_include - headers (cuda_runtime.h, etc.)
      nvidia_sdk_lib     - libraries (libcudart.so, etc.)
    """

    # Read from config, fall back to attrs
    nvidia_sdk_path = read_root_config("nv", "nvidia_sdk_path", ctx.attrs.nvidia_sdk_path)
    nvidia_sdk_include = read_root_config("nv", "nvidia_sdk_include", ctx.attrs.nvidia_sdk_include)
    nvidia_sdk_lib = read_root_config("nv", "nvidia_sdk_lib", ctx.attrs.nvidia_sdk_lib)

    return [
        DefaultInfo(),
        NvToolchainInfo(
            nvidia_sdk_path = nvidia_sdk_path,
            nvidia_sdk_include = nvidia_sdk_include,
            nvidia_sdk_lib = nvidia_sdk_lib,
            nv_archs = ctx.attrs.nv_archs,
        ),
    ]

nv_toolchain = rule(
    impl = _nv_toolchain_impl,
    attrs = {
        # Target NVIDIA architectures
        # sm_90  = Hopper (H100)
        # sm_100 = Blackwell (B100, B200)
        # sm_120 = Blackwell (B200 full features, requires LLVM 22)
        "nv_archs": attrs.list(attrs.string(), default = ["sm_90"]),

        # NVIDIA SDK paths (overridden by .buckconfig.local [nv] section)
        "nvidia_sdk_path": attrs.string(default = "/usr/local/cuda"),
        "nvidia_sdk_include": attrs.string(default = "/usr/local/cuda/include"),
        "nvidia_sdk_lib": attrs.string(default = "/usr/local/cuda/lib64"),
    },
    is_toolchain_rule = True,
)

def nv_compile_flags(nv_toolchain_info: NvToolchainInfo) -> list[str]:
    """
    Generate clang flags for NVIDIA target compilation.

    These flags tell clang to compile .cu files as device code,
    using the nvidia-sdk for headers and the specified SM architectures.
    """
    flags = [
        # Tell clang this is device code
        "-x", "cuda",

        # nvidia-sdk paths (--cuda-path is clang's flag, not CUDA branding)
        "--cuda-path=" + nv_toolchain_info.nvidia_sdk_path,
        "-isystem", nv_toolchain_info.nvidia_sdk_include,

        # C++23 on device
        "-std=c++23",

        # Enable std::mdspan on device
        "-D__MDSPAN_USE_PAREN_OPERATOR=1",
    ]

    # Target architectures
    for arch in nv_toolchain_info.nv_archs:
        flags.extend(["--cuda-gpu-arch=" + arch])

    return flags

def nv_link_flags(nv_toolchain_info: NvToolchainInfo) -> list[str]:
    """
    Generate linker flags for NVIDIA binaries.
    """
    return [
        "-L" + nv_toolchain_info.nvidia_sdk_lib,
        "-Wl,-rpath," + nv_toolchain_info.nvidia_sdk_lib,
        "-lcudart",
    ]

def nv_binary(name: str, srcs: list[str], deps: list[str] = [], visibility: list[str] = []):
    """
    Build an NVIDIA binary using clang (NOT nvcc).

    Compiles .cpp files with CUDA support via clang's native CUDA frontend.
    Reads nvidia-sdk paths from .buckconfig.local [nv] section.
    """
    # Read nvidia-sdk paths from config
    nvidia_sdk_path = read_root_config("nv", "nvidia_sdk_path", "/usr/local/cuda")
    nvidia_sdk_include = read_root_config("nv", "nvidia_sdk_include", "/usr/local/cuda/include")
    nvidia_sdk_lib = read_root_config("nv", "nvidia_sdk_lib", "/usr/local/cuda/lib64")
    
    # Target architectures from config (comma-separated)
    nv_archs_str = read_root_config("nv", "archs", "sm_90")
    arch_flags = ["--cuda-gpu-arch=" + arch.strip() for arch in nv_archs_str.split(",")]

    native.cxx_binary(
        name = name,
        srcs = srcs,
        deps = deps,
        compiler_flags = [
            "-x", "cuda",
            "--cuda-path=" + nvidia_sdk_path,
            "-isystem", nvidia_sdk_include,
            "-std=c++23",
        ] + arch_flags,
        linker_flags = [
            "-L" + nvidia_sdk_lib,
            "-Wl,-rpath," + nvidia_sdk_lib,
            "-lcudart",
        ],
        visibility = visibility,
    )

# Provider for nv_library outputs
NvLibraryInfo = provider(
    doc = "Information about compiled NVIDIA library",
    fields = {
        "objects": provider_field(list),  # List of .o files
        "headers": provider_field(list),  # List of header files
        "include_dir": provider_field(str),  # Directory containing headers
    },
)

def _nv_library_impl(ctx: AnalysisContext) -> list[Provider]:
    """
    Compile CUDA source files into object files.
    
    Uses clang with -x cuda to compile .cu files into position-independent
    object code that can be linked into shared libraries.
    """
    # Get tools from config
    cxx = read_root_config("cxx", "cxx", "clang++")
    nvidia_sdk_path = read_root_config("nv", "nvidia_sdk_path", "/usr/local/cuda")
    nvidia_sdk_include = read_root_config("nv", "nvidia_sdk_include", "/usr/local/cuda/include")
    
    # C++ stdlib paths for unwrapped clang
    gcc_include = read_root_config("cxx", "gcc_include", "")
    gcc_include_arch = read_root_config("cxx", "gcc_include_arch", "")
    glibc_include = read_root_config("cxx", "glibc_include", "")
    clang_resource_dir = read_root_config("cxx", "clang_resource_dir", "")
    
    # Target architectures from config (comma-separated, e.g. "sm_90,sm_100,sm_120")
    nv_archs_str = read_root_config("nv", "archs", "sm_90")
    nv_archs = nv_archs_str.split(",")
    
    # Compile flags for CUDA
    compile_flags = [
        "-x", "cuda",
        "--cuda-path=" + nvidia_sdk_path,
        "-isystem", nvidia_sdk_include,
        "-std=c++17",  # Use c++17 for broader compatibility
        "-fPIC",       # Required for shared library
        "-c",          # Compile only, don't link
    ]
    
    # Add target architectures
    for arch in nv_archs:
        compile_flags.extend(["--cuda-gpu-arch=" + arch.strip()])
    
    # Add stdlib paths for unwrapped clang
    if gcc_include:
        compile_flags.extend(["-isystem", gcc_include])
    if gcc_include_arch:
        compile_flags.extend(["-isystem", gcc_include_arch])
    if glibc_include:
        compile_flags.extend(["-isystem", glibc_include])
    if clang_resource_dir:
        compile_flags.extend(["-resource-dir=" + clang_resource_dir])
    
    # Compile each source file to object
    objects = []
    for src in ctx.attrs.srcs:
        obj_name = src.short_path.replace(".cu", ".o").replace(".cpp", ".o")
        obj = ctx.actions.declare_output(obj_name)
        
        cmd = cmd_args([cxx] + compile_flags + [
            "-o", obj.as_output(),
            src,
        ])
        
        ctx.actions.run(cmd, category = "nv_compile", identifier = src.short_path)
        objects.append(obj)
    
    # Get include directory for headers
    include_dir = ""
    if ctx.attrs.exported_headers:
        # Use the directory containing the first header
        first_header = ctx.attrs.exported_headers[0]
        include_dir = first_header.short_path.rsplit("/", 1)[0] if "/" in first_header.short_path else "."
    
    return [
        DefaultInfo(default_output = objects[0] if objects else None, other_outputs = objects[1:] if len(objects) > 1 else []),
        NvLibraryInfo(
            objects = objects,
            headers = ctx.attrs.exported_headers,
            include_dir = include_dir,
        ),
    ]

nv_library = rule(
    impl = _nv_library_impl,
    attrs = {
        "srcs": attrs.list(attrs.source()),
        "exported_headers": attrs.list(attrs.source(), default = []),
        "deps": attrs.list(attrs.dep(), default = []),
    },
)
