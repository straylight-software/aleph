# nix/build/toolchains/nv.bzl
#
# NVIDIA toolchain using hermetic Nix store paths.
#
# This toolchain provides NVIDIA target compilation via clang (NOT nvcc).
# Paths are read from .buckconfig.local [nv] section, generated by `nix develop`.
#
# We say "nv" not "cuda" because:
#   - "CUDA" is NVIDIA marketing speak
#   - "nv" is explicit about the target hardware
#   - The nvidia-sdk provides headers and runtime, NOT a compiler
#
# Compilation uses clang with:
#   -x cuda          Tell clang this is device code
#   --cuda-path=     Path to nvidia-sdk (headers, libcudart)
#   --cuda-gpu-arch= Target SM architecture (sm_90, sm_120, etc.)
#
# No nvcc. Ever.

NvToolchainInfo = provider(
    doc = "NVIDIA SDK configuration for Clang device compilation",
    fields = {
        "nvidia_sdk_path": provider_field(str),
        "nvidia_sdk_include": provider_field(str),
        "nvidia_sdk_lib": provider_field(str),
        "nv_archs": provider_field(list[str]),
    },
)

def _nv_toolchain_impl(ctx: AnalysisContext) -> list[Provider]:
    """
    NVIDIA toolchain with paths from .buckconfig.local.

    Reads [nv] section for absolute Nix store paths:
      nvidia_sdk_path    - nvidia-sdk root (for --cuda-path)
      nvidia_sdk_include - headers (cuda_runtime.h, etc.)
      nvidia_sdk_lib     - libraries (libcudart.so, etc.)
    """

    # Read from config, fall back to attrs
    nvidia_sdk_path = read_root_config("nv", "nvidia_sdk_path", ctx.attrs.nvidia_sdk_path)
    nvidia_sdk_include = read_root_config("nv", "nvidia_sdk_include", ctx.attrs.nvidia_sdk_include)
    nvidia_sdk_lib = read_root_config("nv", "nvidia_sdk_lib", ctx.attrs.nvidia_sdk_lib)

    return [
        DefaultInfo(),
        NvToolchainInfo(
            nvidia_sdk_path = nvidia_sdk_path,
            nvidia_sdk_include = nvidia_sdk_include,
            nvidia_sdk_lib = nvidia_sdk_lib,
            nv_archs = ctx.attrs.nv_archs,
        ),
    ]

nv_toolchain = rule(
    impl = _nv_toolchain_impl,
    attrs = {
        # Target NVIDIA architectures
        # sm_90  = Hopper (H100)
        # sm_100 = Blackwell (B100, B200)
        # sm_120 = Blackwell (B200 full features, requires LLVM 22)
        "nv_archs": attrs.list(attrs.string(), default = ["sm_90"]),

        # NVIDIA SDK paths (overridden by .buckconfig.local [nv] section)
        "nvidia_sdk_path": attrs.string(default = "/usr/local/cuda"),
        "nvidia_sdk_include": attrs.string(default = "/usr/local/cuda/include"),
        "nvidia_sdk_lib": attrs.string(default = "/usr/local/cuda/lib64"),
    },
    is_toolchain_rule = True,
)

def nv_compile_flags(nv_toolchain_info: NvToolchainInfo) -> list[str]:
    """
    Generate clang flags for NVIDIA target compilation.

    These flags tell clang to compile .cu files as device code,
    using the nvidia-sdk for headers and the specified SM architectures.
    """
    flags = [
        # Tell clang this is device code
        "-x", "cuda",

        # nvidia-sdk paths (--cuda-path is clang's flag, not CUDA branding)
        "--cuda-path=" + nv_toolchain_info.nvidia_sdk_path,
        "-isystem", nv_toolchain_info.nvidia_sdk_include,

        # C++23 on device
        "-std=c++23",

        # Enable std::mdspan on device
        "-D__MDSPAN_USE_PAREN_OPERATOR=1",
    ]

    # Target architectures
    for arch in nv_toolchain_info.nv_archs:
        flags.extend(["--cuda-gpu-arch=" + arch])

    return flags

def nv_link_flags(nv_toolchain_info: NvToolchainInfo) -> list[str]:
    """
    Generate linker flags for NVIDIA binaries.
    """
    return [
        "-L" + nv_toolchain_info.nvidia_sdk_lib,
        "-Wl,-rpath," + nv_toolchain_info.nvidia_sdk_lib,
        "-lcudart",
    ]
